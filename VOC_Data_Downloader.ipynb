{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yepLofzzhbTG",
        "outputId": "dc87a7e7-4bad-4581-ecec-a63b46cdca55"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from threading import Lock\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "print_lock = Lock()\n",
        "\n",
        "def safe_print(*args, **kwargs):\n",
        "    \"\"\"Thread-safe print function.\"\"\"\n",
        "    with print_lock:\n",
        "        print(*args, **kwargs)\n",
        "\n",
        "def download_txt_and_get_filename(url, timeout=30):\n",
        "    \"\"\"\n",
        "    Download TXT file and extract the filename from response.\n",
        "    Returns: (content, filename) or (None, None) on error\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=timeout, stream=True)\n",
        "        response.raise_for_status()\n",
        "        \n",
        "        # Get filename from Content-Disposition header\n",
        "        filename = None\n",
        "        if 'Content-Disposition' in response.headers:\n",
        "            content_disposition = response.headers['Content-Disposition']\n",
        "            # Parse filename from header like: attachment; filename=\"21424.txt\"\n",
        "            match = re.search(r'filename=\"?([^\"]+)\"?', content_disposition)\n",
        "            if match:\n",
        "                filename = match.group(1)\n",
        "        \n",
        "        # If no Content-Disposition, try to extract from URL or response\n",
        "        if not filename:\n",
        "            # Sometimes filename might be in the URL path\n",
        "            filename = url.split('/')[-1]\n",
        "            if not filename.endswith('.txt'):\n",
        "                filename = None\n",
        "        \n",
        "        # Get content\n",
        "        content = response.content.decode('utf-8')\n",
        "        \n",
        "        return content, filename\n",
        "        \n",
        "    except requests.exceptions.RequestException as e:\n",
        "        safe_print(f\"⚠️ Error downloading from {url}: {e}\")\n",
        "        return None, None\n",
        "    except UnicodeDecodeError as e:\n",
        "        safe_print(f\"⚠️ Error decoding content from {url}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def extract_number_from_filename(filename):\n",
        "    \"\"\"Extract the file ID number from the filename.\"\"\"\n",
        "    if not filename:\n",
        "        return None\n",
        "    \n",
        "    # Remove .txt extension and extract number\n",
        "    match = re.search(r'(\\d+)\\.txt$', filename)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    \n",
        "    # Try to find any number in the filename\n",
        "    match = re.search(r'(\\d+)', filename)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    \n",
        "    return None\n",
        "\n",
        "def download_json_file(url, timeout=30):\n",
        "    \"\"\"Download JSON file from URL.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=timeout)\n",
        "        response.raise_for_status()\n",
        "        return response.text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        safe_print(f\"⚠️ Error downloading JSON from {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def parse_transcription_txt(txt_content):\n",
        "    \"\"\"Parse the transcription TXT file and return a dictionary.\"\"\"\n",
        "    transcription_dict = {}\n",
        "    \n",
        "    pattern = r'#\\+\\s*---\\s*\\n#\\+\\s*(NL-HaNA_1\\.04\\.02_\\d+_\\d+)\\.xml\\s*\\n#\\+\\s*---'\n",
        "    matches = list(re.finditer(pattern, txt_content))\n",
        "    \n",
        "    for i, match in enumerate(matches):\n",
        "        current_id = match.group(1)\n",
        "        text_start = match.end()\n",
        "        \n",
        "        if i + 1 < len(matches):\n",
        "            text_end = matches[i + 1].start()\n",
        "        else:\n",
        "            text_end = len(txt_content)\n",
        "        \n",
        "        text_content = txt_content[text_start:text_end].strip()\n",
        "        transcription_dict[current_id] = text_content\n",
        "    \n",
        "    return transcription_dict\n",
        "\n",
        "def merge_transcription_into_manifest(manifest_data, transcription_dict):\n",
        "    \"\"\"Merge transcription text into the manifest canvases.\"\"\"\n",
        "    found_matches = 0\n",
        "    \n",
        "    if 'items' in manifest_data and isinstance(manifest_data['items'], list):\n",
        "        for canvas in manifest_data['items']:\n",
        "            canvas_label = None\n",
        "            \n",
        "            if 'label' in canvas and 'en' in canvas['label'] and canvas['label']['en']:\n",
        "                canvas_label = canvas['label']['en'][0].strip()\n",
        "            \n",
        "            if canvas_label and canvas_label in transcription_dict:\n",
        "                canvas['text'] = transcription_dict[canvas_label]\n",
        "                found_matches += 1\n",
        "            else:\n",
        "                canvas['text'] = \"\"\n",
        "    \n",
        "    return found_matches\n",
        "\n",
        "def process_single_file(txt_url, json_url_template, output_dir):\n",
        "    \"\"\"\n",
        "    Process a single file:\n",
        "    1. Download TXT file and get its filename\n",
        "    2. Extract number from filename\n",
        "    3. Download corresponding JSON using that number\n",
        "    4. Merge and save\n",
        "    \"\"\"\n",
        "    result = {\n",
        "        'txt_url': txt_url,\n",
        "        'file_id': None,\n",
        "        'success': False,\n",
        "        'manifest': None,\n",
        "        'error': None\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        # Download TXT file and get filename\n",
        "        txt_content, txt_filename = download_txt_and_get_filename(txt_url)\n",
        "        \n",
        "        if not txt_content or not txt_filename:\n",
        "            result['error'] = f\"Failed to download TXT or get filename\"\n",
        "            safe_print(f\"⚠️ Failed: {txt_url} - could not download or get filename\")\n",
        "            return result\n",
        "        \n",
        "        # Extract file ID from filename\n",
        "        file_id = extract_number_from_filename(txt_filename)\n",
        "        \n",
        "        if not file_id:\n",
        "            result['error'] = f\"Could not extract number from filename: {txt_filename}\"\n",
        "            safe_print(f\"⚠️ Failed: {txt_url} - filename '{txt_filename}' has no number\")\n",
        "            return result\n",
        "        \n",
        "        result['file_id'] = file_id\n",
        "        result['filename'] = txt_filename\n",
        "        \n",
        "        # Construct JSON URL using the extracted file ID\n",
        "        json_url = json_url_template.format(file_id)\n",
        "        \n",
        "        # Download JSON file\n",
        "        json_content = download_json_file(json_url)\n",
        "        if not json_content:\n",
        "            result['error'] = f\"Failed to download JSON from {json_url}\"\n",
        "            return result\n",
        "        \n",
        "        # Parse JSON\n",
        "        manifest_data = json.loads(json_content)\n",
        "        \n",
        "        # Parse transcription\n",
        "        transcription_dict = parse_transcription_txt(txt_content)\n",
        "        \n",
        "        # Merge transcription into manifest\n",
        "        found_matches = merge_transcription_into_manifest(manifest_data, transcription_dict)\n",
        "        \n",
        "        # Save individual manifest\n",
        "        output_filename = f\"{output_dir}/merged_{file_id}.json\"\n",
        "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(manifest_data, f, indent=2, ensure_ascii=False)\n",
        "        \n",
        "        result['success'] = True\n",
        "        result['manifest'] = manifest_data\n",
        "        result['transcription_count'] = len(transcription_dict)\n",
        "        result['merged_count'] = found_matches\n",
        "        \n",
        "        safe_print(f\"✓ [{file_id}] '{txt_filename}' → {len(transcription_dict)} sections, {found_matches} canvases\")\n",
        "        \n",
        "    except json.JSONDecodeError as e:\n",
        "        result['error'] = f\"JSON parse error: {e}\"\n",
        "        safe_print(f\"⚠️ [{result.get('file_id', '?')}] JSON parse error: {e}\")\n",
        "    except Exception as e:\n",
        "        result['error'] = f\"Processing error: {e}\"\n",
        "        safe_print(f\"⚠️ [{result.get('file_id', '?')}] Error: {e}\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "def create_combined_manifest(all_manifests, file_ids):\n",
        "    \"\"\"Create a comprehensive manifest containing all items.\"\"\"\n",
        "    if not all_manifests:\n",
        "        return None\n",
        "    \n",
        "    combined = {\n",
        "        \"@context\": \"http://iiif.io/api/presentation/3/context.json\",\n",
        "        \"id\": \"https://data.globalise.huygens.knaw.nl/manifests/inventories/combined_all.json\",\n",
        "        \"type\": \"Manifest\",\n",
        "        \"label\": {\n",
        "            \"en\": [f\"Combined Manifest - {len(all_manifests)} Documents\"]\n",
        "        },\n",
        "        \"metadata\": [\n",
        "            {\n",
        "                \"label\": {\"en\": [\"Collection\"]},\n",
        "                \"value\": {\"en\": [\"NL-HaNA VOC Archives\"]}\n",
        "            },\n",
        "            {\n",
        "                \"label\": {\"en\": [\"Total Documents\"]},\n",
        "                \"value\": {\"en\": [str(len(all_manifests))]}\n",
        "            },\n",
        "            {\n",
        "                \"label\": {\"en\": [\"Document IDs\"]},\n",
        "                \"value\": {\"en\": [\", \".join(sorted(file_ids))]}\n",
        "            }\n",
        "        ],\n",
        "        \"rights\": \"https://creativecommons.org/publicdomain/mark/1.0/\",\n",
        "        \"items\": []\n",
        "    }\n",
        "    \n",
        "    canvas_count = 0\n",
        "    for manifest in all_manifests:\n",
        "        if 'items' in manifest:\n",
        "            combined['items'].extend(manifest['items'])\n",
        "            canvas_count += len(manifest['items'])\n",
        "    \n",
        "    safe_print(f\"\\n✓ Combined manifest: {canvas_count} canvases from {len(all_manifests)} documents\")\n",
        "    \n",
        "    return combined\n",
        "\n",
        "def main(tab_file_path, max_workers=15):\n",
        "    \"\"\"\n",
        "    Main processing function.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    tab_file_path : str\n",
        "        Path to the .tab file containing TXT download URLs\n",
        "    max_workers : int\n",
        "        Number of concurrent workers (default: 15)\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    print(\"IIIF Manifest Batch Processor (Local Jupyter)\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Check if tab file exists\n",
        "    if not os.path.exists(tab_file_path):\n",
        "        print(f\"❌ Error: File not found: {tab_file_path}\")\n",
        "        return\n",
        "    \n",
        "    print(f\"\\n✓ Reading from: {tab_file_path}\")\n",
        "    \n",
        "    # Read URLs from file\n",
        "    with open(tab_file_path, 'r', encoding='utf-8') as f:\n",
        "        tab_content = f.read()\n",
        "    \n",
        "    txt_urls = [line.strip() for line in tab_content.split('\\n') if line.strip()]\n",
        "    \n",
        "    print(f\"✓ Found {len(txt_urls)} TXT download URLs\")\n",
        "    \n",
        "    # JSON URL template\n",
        "    json_url_template = \"https://data.globalise.huygens.knaw.nl/manifests/inventories/{}.json\"\n",
        "    \n",
        "    # Create output directory\n",
        "    output_dir = \"merged_manifests\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f\"✓ Output directory: {output_dir}/\")\n",
        "    \n",
        "    # Process files concurrently\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Processing {len(txt_urls)} files concurrently...\")\n",
        "    print(f\"Step 1: Download TXT files and extract filenames\")\n",
        "    print(f\"Step 2: Use filename numbers to download JSON manifests\")\n",
        "    print(f\"Step 3: Merge transcriptions and save\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    all_results = []\n",
        "    \n",
        "    # Concurrent processing\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        future_to_url = {\n",
        "            executor.submit(\n",
        "                process_single_file,\n",
        "                txt_url,\n",
        "                json_url_template,\n",
        "                output_dir\n",
        "            ): txt_url\n",
        "            for txt_url in txt_urls\n",
        "        }\n",
        "        \n",
        "        completed = 0\n",
        "        for future in as_completed(future_to_url):\n",
        "            txt_url = future_to_url[future]\n",
        "            try:\n",
        "                result = future.result()\n",
        "                all_results.append(result)\n",
        "                completed += 1\n",
        "                \n",
        "                if completed % 5 == 0:\n",
        "                    safe_print(f\"Progress: {completed}/{len(txt_urls)} completed...\")\n",
        "                    \n",
        "            except Exception as e:\n",
        "                safe_print(f\"⚠️ Unexpected error for {txt_url}: {e}\")\n",
        "                all_results.append({\n",
        "                    'txt_url': txt_url,\n",
        "                    'file_id': None,\n",
        "                    'success': False,\n",
        "                    'manifest': None,\n",
        "                    'error': str(e)\n",
        "                })\n",
        "    \n",
        "    elapsed_time = time.time() - start_time\n",
        "    \n",
        "    # Separate results\n",
        "    successful_results = [r for r in all_results if r['success']]\n",
        "    failed_results = [r for r in all_results if not r['success']]\n",
        "    \n",
        "    all_manifests = [r['manifest'] for r in successful_results]\n",
        "    successful_ids = [r['file_id'] for r in successful_results]\n",
        "    \n",
        "    # Create combined manifest\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"Creating combined manifest...\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    combined_manifest = create_combined_manifest(all_manifests, successful_ids)\n",
        "    \n",
        "    if combined_manifest:\n",
        "        combined_filename = f\"{output_dir}/combined_all_manifests.json\"\n",
        "        with open(combined_filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(combined_manifest, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"✓ Saved: {combined_filename}\")\n",
        "    \n",
        "    # Create ZIP\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"Creating ZIP archive...\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    zip_filename = \"merged_manifests.zip\"\n",
        "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files_list in os.walk(output_dir):\n",
        "            for file in files_list:\n",
        "                file_path = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(file_path, output_dir)\n",
        "                zipf.write(file_path, arcname)\n",
        "    \n",
        "    print(f\"✓ Created: {zip_filename}\")\n",
        "    \n",
        "    # Summary\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"PROCESSING COMPLETE\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"⏱️  Total time: {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)\")\n",
        "    print(f\"⚡ Avg speed: {elapsed_time/len(txt_urls):.2f} seconds per file\")\n",
        "    print(f\"✓ Successfully processed: {len(successful_results)} files\")\n",
        "    print(f\"✗ Failed: {len(failed_results)} files\")\n",
        "    \n",
        "    if successful_results:\n",
        "        print(f\"\\nSuccessfully processed IDs: {', '.join(sorted(successful_ids)[:10])}\", end=\"\")\n",
        "        if len(successful_ids) > 10:\n",
        "            print(f\"... (+{len(successful_ids)-10} more)\")\n",
        "        else:\n",
        "            print()\n",
        "    \n",
        "    if failed_results:\n",
        "        print(f\"\\nFailed files:\")\n",
        "        for result in failed_results[:10]:\n",
        "            file_id = result.get('file_id', '?')\n",
        "            error = result.get('error', 'Unknown error')\n",
        "            print(f\"  [{file_id}] {error}\")\n",
        "        if len(failed_results) > 10:\n",
        "            print(f\"  ... and {len(failed_results) - 10} more\")\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Output files saved in current directory:\")\n",
        "    print(f\"  • {output_dir}/ - Individual manifests ({len(all_manifests)} files)\")\n",
        "    print(f\"  • {output_dir}/combined_all_manifests.json - Combined manifest\")\n",
        "    print(f\"  • {zip_filename} - ZIP archive of all files\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(\"\\n✓ Done!\")\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# Uncomment and modify the path to your .tab file\n",
        "\n",
        "main('globalise_transcriptions_v2_txt.tab')\n",
        "\n",
        "# Or with custom number of workers:\n",
        "# main('urls.tab', max_workers=20)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
